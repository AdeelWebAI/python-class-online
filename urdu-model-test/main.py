from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "enstazao/Qalb-1.0-8B-Instruct",
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True, # <--- Currently set to use 4-bit quantization
)
FastLanguageModel.for_inference(model)


urdu_system_prompt = "Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø§ÙˆØ± Ø¨Û’ Ø¶Ø±Ø± Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Û’ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ” Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ø¯Ø±Ø³Øª Ø¬ÙˆØ§Ø¨Ø§Øª Ø¯ÛŒØªÛ’ ÛÛŒÚºÛ”"

questions = [
    "Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ú©Ú¾ÛŒÙ„ Ú©ÛŒØ§ ÛÛ’ØŸ",                         
    "Ù„Ø§ÛÙˆØ± Ø´ÛØ± Ú©ÛŒÙˆÚº Ù…Ø´ÛÙˆØ± ÛÛ’ØŸ Ù…Ø®ØªØµØ± ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚºÛ”",
    "Ø³ÙˆØ§Ù„: Ù„ÛŒØ§Ù‚Øª Ø¹Ù„ÛŒ Ø®Ø§Ù† Ú©ÙˆÙ† ØªÚ¾Û’ØŸ",
    "Ú©Ø±Ø§Ú†ÛŒ Ú©Ùˆ Ø±ÙˆØ´Ù†ÛŒÙˆÚº Ú©Ø§ Ø´ÛØ± Ú©ÛŒÙˆÚº Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŸ",             
    "Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚº: 'Ù…Ø­Ù†Øª Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ú©Ù†Ø¬ÛŒ ÛÛ’Û”'"
]

print("ðŸš€ Starting Batch Generation...\n")


for user_input in questions:
    print(f"ðŸ”¹ Question: {user_input}")

    # Manually Format Prompt (Llama-3 Style)
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{urdu_system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

    inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens = 256,
        temperature = 0.1,
        top_p = 0.9,
        repetition_penalty = 1.1,
        do_sample = True,
        eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
    )

    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    
    print(f"âœ… Answer: {response}")
    print("-" * 50)
